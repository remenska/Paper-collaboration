
\documentclass[10pt,conference]{IEEEtran}

\usepackage{url}
\usepackage{graphicx}
\usepackage{cite}
%\usepackage[center]{caption}
\usepackage{authblk}
\begin{document}
\title{Behavioral Analysis of the Agent-Based Community Grid Solution for the Large Hadron Collider beauty Experiment}
\author[1,3]{Daniela Remenska}
\author[2]{Tim A.C. Willemse}
\author[1]{Henri Bal}
\author[1]{Kees Verstoep}
\author[1]{Wan Fokkink}
\author[3]{Jeff Templon}

\affil[1]{Dept. of Computer Science, VU University Amsterdam, The Netherlands}
\affil[2]{Dept. of Computer Science, TU Eindhoven, The Netherlands}
\affil[3]{NIKHEF, Amsterdam, The Netherlands}

\maketitle

\begin{abstract}
DIRAC (Distributed Infrastructure with Remote Agent Control) is the Grid
solution designed to support production activities as well as user data analysis
for the Large Hadron Collider beauty experiment. It consists of cooperating
distributed services and a plethora of light-weight agents delivering the
workload to the Grid resources.  Services accept requests from agents and
running jobs, while agents actively fulfill specific goals. Services maintain
database back-ends to store dynamic state information of entities such as jobs,
queues, or staging requests. Agents continuously check for changes in the
service states, and react to these accordingly. The logic of each agent is
rather simple; the main source of complexity lies in their cooperation. These
agents run concurrently, and communicate using the services' databases as a
shared memory for synchronizing the state transitions. Despite the effort
invested in making DIRAC reliable, entities occasionally get into inconsistent
states. Tracing and fixing such behaviors is difficult, given the inherent
parallelism among the distributed components, and the size of the
implementation.

In this paper we present an analysis of DIRAC with mCRL2, process algebra with
data. We have reverse engineered two critical and related DIRAC subsystems, and
subsequently modeled their behavior with the mCRL2 toolset. This enabled us to
easily locate race conditions and livelocks which were confirmed to occur in the
real system. We further formalized and verified several behavioral properties of
the two modeled subsystems.

\textbf{Keywords:} DIRAC, service-oriented architecture, agents, stager,
mCRL2,  model checking,  process algebra

\end{abstract}


\section{Introduction}

The Large Hadron Collider beauty (LHCb) experiment \cite{LHCb} is one of the
four large
experiments conducted on the Large Hadron Collider (LHC) accelerator, built by
the European Organization for Nuclear Research (CERN). Immense amounts of data
are produced at the LHC accelerator, and subsequently processed by physics
groups and individuals worldwide. The sheer size of the experiment is the
motivation behind the adoption of the Grid computing paradigm. The Grid storage
and computing resources for the LHCb experiment are distributed across several
institutes in Europe. To cope with the complexity of processing the vast amount
of data, a complete Grid solution, called DIRAC (Distributed Infrastructure with
Remote Agent Control) \cite{DIRAC_CommGridSolution},
\cite{DIRAC_ReliableDataMaangement}, has been designed and developed for the
LHCb
community.

DIRAC forms a layer between the LHCb community of users and the heterogeneous
Grid resources, in order to allow for optimal and reliable usage of these
resources. It consists of many cooperating distributed services and light-weight
agents which deliver workload to the resources. The logic of each individual
component is relatively simple; the overall system complexity emerges from the
cooperation among them. Namely, these agents run concurrently, and communicate
using the services' databases as a shared memory (blackboard paradigm \cite{IEEEexample:blackboard_systems}) for
synchronizing state transitions of various entities.

Although much effort is invested in making DIRAC reliable, entities occasionally
get into inconsistent states, leading to a potential loss of efficiency in both
resource usage and manpower. Debugging and fixing the root of such encountered
behaviors becomes a formidable mission due to multiple factors: the inherent
parallelism present among the system components deployed on different physical
machines, the size of the implementation (around 150000 lines of Python code),
and the distributed knowledge of different subsystems within the collaboration.

In this paper we propose the use of more rigorous (formal) methods for improving
software quality. Model checking \cite{ProcessesWithData} is one such technique for analysis of an
abstract model of a system, and verification of certain system properties of
interest. Unlike conventional testing, it allows full control over the execution
of parallel processes and also supports automated exhaustive state-space
exploration.

We used the mCRL2 language \cite{FormalLanguagemCRL2} and toolset \cite{mCRL2Toolset} to model the behavior of two
critical and related DIRAC components: the Workload Management (WMS) and the
Storage Management System (SMS). Based on Algebra of Communicating Processes
(ACP) \cite{process_algebra}, mCRL2 is able to deal with generic data types as well as user-defined
functions for data transformation. This makes it particularly suitable for
modeling the data manipulations made by DIRAC's agents. Visualizing the state
space and replaying scenarios with the toolkit's simulator enabled us to gain
insight into the system behavior, incrementally improve the model, and to
already detect critical race-conditions and livelocks, which were confirmed to
occur in the real system. Some of them were a result of simple coding bugs;
others unveiled more elementary design problems. We further formulated,
formalized and verified several general and application-specific properties.

The idea of modeling existing systems using 
formal techniques is as such not new. Earlier studies
(\cite{SPIN_case_study,sessionMgmtMasterThesis,desing_validation_protocols,protocol_verification_muCRL,SLAMToolkit,SlidingWindowProtocol,DHCP_SPIN}) mostly focused
on modeling and verifying hardware or communication protocols, since
the formal languages and tools at hand were not sufficiently mature
to cope with more complex data-intensive distributed systems. More
recently, success stories on modeling real-life concurrent systems with
data have been reported (\cite{CMS_LHC, Linux_driver, SystemC_processAlgebra, Java_PathFinder, ACS_mCRL2}).
In \cite{SystemC_processAlgebra} the authors have implemented a tool for automatic
translation of the SystemC language into mCRL2 statements. This greatly
simplifies the analysis, but has so far been feasible only when the
language of implementation is domain-specific, or alternatively, a
reasonably small subset of a general-purpose language is considered for
translation. The only exception in this respect is the Java Pathfinder
tool \cite{Java_PathFinder} used to find deadlocks and other behavioral properties
in Java software systems developed by NASA.

We believe that the challenges and results of this work are unique in
a number of aspects. First, to the best of our knowledge, the code-base
and the number of concurrent components engaged in providing DIRAC’s
functionality considerably outnumber previous industrial cases. Second,
the choice of Python as implementation platform has lead to prevailing
usage of dynamic structures (whose types and sizes are determined at
runtime) throughout DIRAC, challenging the transition to an abstract
formal representation. We have nevertheless established general
guidelines on extracting a model outline from the implementation. Third,
analysis of this kind is typically performed after a problem has already
surfaced in the real system, as a means to understand the events which
lead to it and test for possible solutions. We managed to stumble on an
actual bug at the same time it was observed in practice, which increased
our confidence in the soundness of the model.

The paper is organized as follows. Section 2 introduces the architecture
of DIRAC, focusing on the two studied subsystems. Section 3 gives a brief
overview of the mCRL2 language, and describes our approach to abstracting
and modeling the behavior of these subsystems. Section 4 presents the
analysis with the mCRL2 toolset and the issues detected. Section 5
concludes and discusses future work. 

\section{DIRAC: A Community Grid Solution}
\label{sec:Section_2}
\subsection{Architecture Overview}

The development of DIRAC started in 2002 as a system for production of simulation
data that would serve to verify theory, aspects of the LHCb detector design, as
well as to optimize algorithms. It gradually evolved into a complete community
Grid solution for data and job management, based on a general-purpose framework
that can be reused by other communities besides LHCb. Today, it covers all major
LHCb tasks starting with the raw data transfer from the experiment’s detector to
the Grid storage, several steps of data processing, up to the final user
analysis. Python was chosen as the implementation language, since it enables
rapid prototyping and development of new features. DIRAC follows the Service
Oriented Architecture (SOA) paradigm, accompanied by a network of lightweight
distributed agents which animate the system. Its main components are depicted in
Fig.~\ref{fig:DIRAC-Arch}. 

\begin{figure}[t]
\includegraphics[width=0.85\linewidth,keepaspectratio=true]{./DIRAC_Architecture.png}
\centering
\caption{DIRAC Architecture overview}
\label{fig:DIRAC-Arch}
\end{figure}

The \textit{services} are passive components that react to requests from their clients,
possibly soliciting other services in order to fulfill the requests. They run as
permanent processes deployed on a number of high-availability hosts (VO-boxes)
at CERN, and persist the dynamic system state information in database
repositories. The user interfaces, agents or running jobs can act as clients
placing the requests to DIRAC’s services.

\textit{Agents} are active components that fulfill a limited number of specific system
functions. They can run in different environments, depending on their mission.
Some are deployed close to the corresponding services, while others run on the
Grid worker nodes.  Examples of the later are the so-called Pilot Agents, part
of the Workload Management System explained in the following section. All DIRAC
agents repeat the same logic in each iteration cycle: they observe for changes
in the service states, and react accordingly by initiating actions (like job
submission or data transfer) which may update the states of various system
entities.

\textit{Resources} are software abstractions of the underlying heterogeneous Grid
computing and storage resources allocated to LHCb, providing a uniform interface
for access. The physical resources are controlled by the site managers and made
available through middleware services such as gLite \cite{gLite}.

The DIRAC functionality is exposed to users and developers through a rich set of
command-line tools forming the DIRAC API, complemented by a Web portal for
visual monitoring the system behavior and controlling the ongoing tasks. Both
the Web and command-line \textit{interfaces} ensure secure system access using X509
certificates.

\subsection{Workload Management System}

The driving force of DIRAC is the Workload Management System (WMS). Taking into
account that the distributed computing environment is intrinsically unstable, a
\textit{Pilot Job paradigm}, illustrated in Fig.~\ref{fig:DIRAC-WMS}, was chosen as an efficient way to
implement a pull scheduling mechanism.  Pilot jobs are simply resource
reservation processes without an actual payload defined apriori. They are
submitted to the Grid worker nodes with the aim of checking the sanity of the
operational environment just before pulling and executing the real payload. This
hides the fragility of the underlying resources and increases the job success
rate, from the perspective of end users Centralized \textit{Task Queues} contain all the
pending jobs, organized in groups according to their requirements. This enables
efficient application of job priorities, enforcing faire-share policies and
quotas, which are decided within the LHCb community, without the need of any
effort from the Grid sites.  
\begin{figure}[t]
\includegraphics[width=\linewidth,keepaspectratio=true]{./DIRAC_WMS.png}
\centering
\caption{DIRAC Workload Management System \cite{DIRAC_pilot_WMS}}
\label{fig:DIRAC-WMS}
\end{figure}
 \begin{figure}[b]
\includegraphics[width=\linewidth,keepaspectratio=true]{./dirac-primary-states.png}
\centering
\caption{Job state machine \cite{ProductionShifterGuide}}
\label{fig:DIRAC-job-state-machine}
\end{figure} 

The basic flowchart describing the evolution of a job’s states is depicted in
Fig.~\ref{fig:DIRAC-job-state-machine}. After submission through the \textit{Job Manager} (“Received”), the complete
job description is placed in the DIRAC job repository (the Job DB). Before jobs
become eligible for execution, a chain of optimizer agents check and prioritize
them in queues, utilizing the parameters information from the Job DB. \textit{The Job
Sanity Agent} can meaningfully fail jobs with impossible requirements, and
prevent unnecessary submission to the Grid resources.The \textit{Input Data Agent}
performs a resolution of possible target computing resources based on the input
data requirements. If the requested data resides on tape storage, the \textit{Job
Scheduling Agent} will pass the control to a specialized \textit{Stager} service (part of
the SMS explained in the next section), before placing the job a Task Queue
(“Waiting”). Based on the complete list of pending payloads, a specialized \textit{Task
Queue Director} submits pilots to the computing resources via the gLite WMS. After
a \textit{Matcher} service pulls the most suitable payload for a pilot (“Matched”), a \textit{Job
Wrapper} object is created on the worker node, responsible for retrieving the
input sandbox, performing software availability checks, executing the actual
payload on the worker node (“Running”), and finally uploading any output data
necessary (“Done” or “Failed”). The wrapper can catch any failure exit state
reported by the running physics applications. At the same time, a \textit{Watchdog}
process is instantiated to monitor the behavior of the Job Wrapper and send
heartbeat signals to the monitoring service. It can also take actions in case
resources are soon to be exhausted, the payload stalls, or a management command
for killing the payload is received.

Although the Grid storage resources are limited, it is essential to keep all
data collected throughout the experiment’s run. Tape backends provide a reliable
and cheap solution for data storage. The additional workflow step 
necessary for input data files residing on tape is carried out inside the
Storage Management System (SMS).

\subsection{Storage Management System}

The DIRAC SMS provides the logic for pre-staging files from tape to a disk cache
frontend, before a job is able to process them. Smooth functioning of this
system is essential for production activities which involve reprocessing of
older data with improved physics software, and happens typically several times
per year.

A simplified view of the system is shown in  Fig.~\ref{fig:DIRAC-SMS}. The workflow is initiated
with the Job Scheduling agent detecting that a job is assigned to process files
only available on tape storage. It sends a request for staging (i.e., creating a
cached replica) to the \textit{Storage Manager Handler} service along with the
list of files and a callback method to be invoked when the request has been
processed. The Storage Management DB is immediately populated with records which
are processed by a sequence of agents in an organized fashion. The relevant
tables in the SMS DB are the \textit{Tasks} and \textit{CacheReplicas}, whose
entities maintain a state observed and updated by these agents. Tasks maintain
general information about every job requesting a service from the SMS. The
details about every file (i.e., the Storage Element (SE) where it resides, the
size, checksum, number of tasks that requested it), are kept in the CacheReplicas
table. Other auxiliary tables maintain the relationship between these entities.
\begin{figure}[t]
\includegraphics[width=0.9\linewidth,keepaspectratio=true]{./SMS_simplified.png}
\centering
\caption{DIRAC Storage Management System}
\label{fig:DIRAC-SMS}
\end{figure}
The processing begins with the \textit{Request Preparation Agent}. It selects all the
“New” entries for files, checks whether they are registered in a special logical
file catalog, and retrieves their metadata. In case of problematic catalog
entries, it can update the status of the CacheReplicas and the related Tasks
entries to “Failed”. For non-problematic files it carries out transition to
“Waiting”. The \textit{StageRequest Agent} is responsible for placing the actual staging
requests for all “Waiting” entries, via dedicated storage middleware that
communicates with the tape backends. These requests are grouped by SE prior to
submission, and carry information about the requested (pin) lifetime of the
replicas to be cached. If certain pathologies are discovered (i.e., lost,
unavailable or zero-sized files on tape), it can update the corresponding
entries to “Failed” in a similar manner. Otherwise, they are promoted to
“StageSubmitted”. The agent responsible for monitoring the status of submitted
requests is the \textit{StageMonitor Agent}. It achieves this by interrogating the
storage middleware to see if the “StageSubmitted” files are successfully
replicated on disk cache. In case of success, the CacheReplicas and their
corresponding Tasks entries are updated to “Staged”. Various circumstances of
tape or middleware misbehavior can also fail the staging requests. The last one
in the chain is the \textit{RequestFinalization Agent}. The tasks which are in their
final states (“Staged” or “Failed”) are cleared from the database, and callbacks
are performed to the WMS, which effectively wakes up the corresponding jobs. If
there are no more associated tasks for particular replicas, the respective
CacheReplicas entries are also removed.

This subsystem is still in evolutionary phase. In multiple instances, tasks or
replicas have become “stuck”, effectively blocking the progress of jobs. Tracing
back the sequence of events which led to the inconsistent states is non-trivial.
To temporarily alleviate such problems, the status of these entries is typically
reset to the initial “New” state manually, so agents can re-process them from
scratch. Occasionally, error messages are reported from unsuccessful attempts of
the SMS service to update the state of a non-existent table entry.

\bibliographystyle{IEEEtran} 
\bibliography{myLibrary}

\end{document}


